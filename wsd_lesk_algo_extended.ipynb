{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/polock11/nlp_fall_2023/blob/main/wsd_lesk_algo_extended.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFHBYx58GHFO",
        "outputId": "3cbcb6f7-be22-4caf-f6fe-f9dbcd0b3197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sense: Synset('difficult.a.01')\n",
            "definition: not easy; requiring great physical or mental effort to accomplish or comprehend or endure\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "##########################################\n",
        "def extend_sig(sig):\n",
        "    lemma_names_set = set()\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    sig = sig - stop_words\n",
        "\n",
        "\n",
        "    for term in sig:\n",
        "        lemma_names_set.add(term)\n",
        "\n",
        "        synsets = wordnet.synsets(term)\n",
        "\n",
        "        # for each synset, find its hyponyms and hypernyms\n",
        "        for synset in synsets:\n",
        "            hyponyms_set = set()\n",
        "            hypernyms_set = set()\n",
        "\n",
        "            hyponyms_set.update(synset.hyponyms())\n",
        "            hypernyms_set.update(synset.hypernyms())\n",
        "\n",
        "            hyponyms_and_hypernyms_set = hyponyms_set.union(hypernyms_set)\n",
        "\n",
        "            for synset in hyponyms_and_hypernyms_set:\n",
        "                for lemma in synset.lemmas():\n",
        "                    l = lemma.name().lower().replace('_', ' ').replace('-', ' ')\n",
        "\n",
        "                    words = l.split()\n",
        "\n",
        "                    lemma_names_set.update(words)\n",
        "\n",
        "    # return extended signature\n",
        "    return lemma_names_set\n",
        "\n",
        "##########################################\n",
        "\n",
        "\"The exam was too hard for the studensts to pass\"\n",
        "'I am going to the bank with some money'\n",
        "\n",
        "cotext_sentence = \"The exam was too hard for the studensts to pass\"\n",
        "amb_word = \"hard\"\n",
        "\n",
        "max_overlap = 0\n",
        "lesk_sense = None\n",
        "lesk_definition = ''\n",
        "context_words = nltk.word_tokenize(cotext_sentence)\n",
        "context_words = set(context_words)\n",
        "\n",
        "for sense in wn.synsets(amb_word):\n",
        "    #print(f\"sense: {sense}\")\n",
        "    signature = set()\n",
        "    sense_definitions = nltk.word_tokenize(sense.definition())\n",
        "    #print(f'sense_definitions : {sense_definitions}')\n",
        "    signature = signature.union(set(sense_definitions))\n",
        "    signature = signature.union(set(sense.lemma_names()))\n",
        "    #print(f\"examples: {sense.examples()}\")\n",
        "    for example in sense.examples():\n",
        "        signature = signature.union(set(example.split()))\n",
        "    #print(f\"signature: {signature}\")\n",
        "    signature = extend_sig(signature)\n",
        "    #print(f\"extended_signature: {signature}\")\n",
        "    overlap  = len(context_words.intersection(signature))\n",
        "    if overlap > max_overlap:\n",
        "        lesk_sense = sense\n",
        "        max_overlap = overlap\n",
        "        lesk_definition = sense.definition()\n",
        "\n",
        "print(f\"Sense: {lesk_sense}\")\n",
        "print(f\"definition: {lesk_definition}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "##########################################\n",
        "def extend_sig(sig):\n",
        "    lemma_names_set = set()\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    sig = sig - stop_words\n",
        "\n",
        "\n",
        "    for term in sig:\n",
        "        lemma_names_set.add(term)\n",
        "\n",
        "        synsets = wordnet.synsets(term)\n",
        "\n",
        "        # for each synset, find its hyponyms and hypernyms\n",
        "        for synset in synsets:\n",
        "            hyponyms_set = set()\n",
        "            hypernyms_set = set()\n",
        "\n",
        "            hyponyms_set.update(synset.hyponyms())\n",
        "            hypernyms_set.update(synset.hypernyms())\n",
        "\n",
        "            hyponyms_and_hypernyms_set = hyponyms_set.union(hypernyms_set)\n",
        "\n",
        "            for synset in hyponyms_and_hypernyms_set:\n",
        "                for lemma in synset.lemmas():\n",
        "                    l = lemma.name().lower().replace('_', ' ').replace('-', ' ')\n",
        "\n",
        "                    words = l.split()\n",
        "\n",
        "                    lemma_names_set.update(words)\n",
        "\n",
        "    # return extended signature\n",
        "    return lemma_names_set\n",
        "\n",
        "##########################################\n",
        "\n",
        "\"The exam was too hard for the studensts to pass\"\n",
        "'I am going to the bank with some money'\n",
        "\n",
        "cotext_sentence = \"The exam was too hard for the studensts to pass\"\n",
        "amb_word = \"hard\"\n",
        "\n",
        "max_overlap = 0\n",
        "lesk_sense = None\n",
        "lesk_definition = ''\n",
        "context_words = nltk.word_tokenize(cotext_sentence)\n",
        "context_words = set(context_words)\n",
        "\n",
        "for sense in wn.synsets(amb_word):\n",
        "    #print(f\"sense: {sense}\")\n",
        "    signature = set()\n",
        "    sense_definitions = nltk.word_tokenize(sense.definition())\n",
        "    #print(f'sense_definitions : {sense_definitions}')\n",
        "    signature = signature.union(set(sense_definitions))\n",
        "    signature = signature.union(set(sense.lemma_names()))\n",
        "    #print(f\"examples: {sense.examples()}\")\n",
        "    for example in sense.examples():\n",
        "        signature = signature.union(set(example.split()))\n",
        "    #print(f\"signature: {signature}\")\n",
        "    signature = extend_sig(signature)\n",
        "    #print(f\"extended_signature: {signature}\")\n",
        "    overlap  = len(context_words.intersection(signature))\n",
        "    if overlap > max_overlap:\n",
        "        lesk_sense = sense\n",
        "        max_overlap = overlap\n",
        "        lesk_definition = sense.definition()\n",
        "\n",
        "print(f\"Sense: {lesk_sense}\")\n",
        "print(f\"definition: {lesk_definition}\")"
      ],
      "metadata": {
        "id": "MAgIB_U8Hsaq",
        "outputId": "2d85bc45-9793-497a-9518-7cd9960c0235",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sense: Synset('difficult.a.01')\n",
            "definition: not easy; requiring great physical or mental effort to accomplish or comprehend or endure\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# python\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from flask import Flask, jsonify, request, abort\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "sample1 = 'The exam was too hard for the students to pass'\n",
        "sample2 = 'I am going to the bank with some money'\n",
        "\n",
        "\n",
        "def extend_signature(signature):\n",
        "    lemma_names_set = set()\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    signature = signature - stop_words\n",
        "\n",
        "    for term in signature:\n",
        "        lemma_names_set.add(term)\n",
        "        synsets = wordnet.synsets(term)\n",
        "\n",
        "        # for each synset, find its hyponyms and hypernyms\n",
        "        for synset in synsets:\n",
        "            hyponyms_set = set()\n",
        "            hypernyms_set = set()\n",
        "\n",
        "            hyponyms_set.update(synset.hyponyms())\n",
        "            hypernyms_set.update(synset.hypernyms())\n",
        "\n",
        "            hyponyms_and_hypernyms_set = hyponyms_set.union(hypernyms_set)\n",
        "\n",
        "            for synset in hyponyms_and_hypernyms_set:\n",
        "                for lemma in synset.lemmas():\n",
        "                    l = lemma.name().lower().replace('_', ' ').replace('-', ' ')\n",
        "                    words = l.split()\n",
        "                    lemma_names_set.update(words)\n",
        "\n",
        "    # return extended signature\n",
        "    return lemma_names_set\n",
        "\n",
        "\n",
        "def extended_lesk(context_sentence, ambigious_word):\n",
        "    max_overlap = 0\n",
        "    lesk_sense = None\n",
        "    lesk_definition = ''\n",
        "    context_words = nltk.word_tokenize(context_sentence)\n",
        "    context_words = set(context_words)\n",
        "\n",
        "    for sense in wordnet.synsets(ambigious_word):\n",
        "        # print(f\"sense: {sense}\")\n",
        "        signature = set()\n",
        "        sense_definitions = nltk.word_tokenize(sense.definition())\n",
        "        # print(f'sense_definitions : {sense_definitions}')\n",
        "        signature = signature.union(set(sense_definitions))\n",
        "        signature = signature.union(set(sense.lemma_names()))\n",
        "        # print(f\"examples: {sense.examples()}\")\n",
        "        for example in sense.examples():\n",
        "            signature = signature.union(set(example.split()))\n",
        "        # print(f\"signature: {signature}\")\n",
        "        signature = extend_signature(signature)\n",
        "        # print(f\"extended_signature: {signature}\")\n",
        "        overlap = len(context_words.intersection(signature))\n",
        "        if overlap > max_overlap:\n",
        "            lesk_sense = sense\n",
        "            max_overlap = overlap\n",
        "            lesk_definition = sense.definition()\n",
        "            return {\n",
        "                sense: lesk_sense,\n",
        "                overlap: overlap,\n",
        "                definition: lesk_definition\n",
        "            }\n",
        "\n",
        "\n",
        "def lesk(context_sentence, ambigious_word):\n",
        "    # implement method here\n",
        "    pass\n",
        "    # return {\n",
        "    #     sense: lesk_sense,\n",
        "    #     overlap: overlap,\n",
        "    #     definition: lesk_definition\n",
        "    # }\n",
        "\n",
        "\n",
        "@app.route('/lesk', methods=['GET'])\n",
        "def lesk():\n",
        "    context_sentence = request.args.get('context_sentence')\n",
        "    ambiguous_word = request.args.get('ambiguous_word')\n",
        "\n",
        "    if not context_sentence or not ambiguous_word:\n",
        "        return jsonify({\"error\": \"Both 'context_sentence' and 'ambiguous_word' parameters are required.\"}), 400\n",
        "\n",
        "    result = lesk(context_sentence, ambiguous_word)\n",
        "    response = jsonify(result)\n",
        "    response.headers['Content-Type'] = 'application/json'\n",
        "    return response\n",
        "\n",
        "\n",
        "@app.route('/extended_lesk', methods=['GET'])\n",
        "def extended_lesk():\n",
        "    context_sentence = request.args.get('context_sentence')\n",
        "    ambiguous_word = request.args.get('ambiguous_word')\n",
        "\n",
        "    if not context_sentence or not ambiguous_word:\n",
        "        return jsonify({\"error\": \"Both 'context_sentence' and 'ambiguous_word' parameters are required.\"}), 400\n",
        "\n",
        "    result = extended_lesk(context_sentence, ambiguous_word)\n",
        "    response = jsonify(result)\n",
        "    response.headers['Content-Type'] = 'application/json'\n",
        "    return response\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ],
      "metadata": {
        "id": "SOEoIwxDOZbO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}